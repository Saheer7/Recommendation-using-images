{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image resizing ( Dimensions of images to 200*200)\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = \"C:/Users/Nitin/Desktop/Capilary/train/images/\"\n",
    "\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "def resize():\n",
    "    for item in dirs:\n",
    "        if os.path.isfile(path+item):\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(path+item)\n",
    "            imResize = im.resize((119,178), Image.ANTIALIAS) # 1/3 rd of both the original dimensions\n",
    "            g=f.replace('images','resized_images')\n",
    "            imResize.save(g + ' resized.jpg', 'JPEG', quality=90)\n",
    "\n",
    "resize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Nitin/Desktop/Capilary/train/resized_images/')\n",
    "path = \"C:/Users/Nitin/Desktop/Capilary/train/resized_images/\"\n",
    "dirs = os.listdir( path )\n",
    "x=[]\n",
    "for item in dirs:\n",
    "    img=cv2.imread(item)\n",
    "    imgarr= np.array(img)\n",
    "    x.append(imgarr.astype('float32')/255.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "import random\n",
    "random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test\n",
    "x_train = x[0:round(0.9*len(x))]\n",
    "x_test =  x[round(0.9*len(x)):len(x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.array(x_train)\n",
    "x_test= np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2723, 178, 119, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Input, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, UpSampling2D, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 178, 119, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 178, 119, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 89, 59, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 89, 59, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 44, 29, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10208)             0         \n",
      "_________________________________________________________________\n",
      "encoder (Dense)              (None, 512)               5227008   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10208)             5236704   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 44, 29, 8)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 88, 58, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 88, 58, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 176, 116, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 176, 116, 16)      1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 178, 119, 3)       579       \n",
      "=================================================================\n",
      "Total params: 10,467,651\n",
      "Trainable params: 10,467,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define the deep learning architecture\n",
    "input_img = Input(shape=(178,119,3))  # adapt this if using `channels_first` image data format\n",
    "x= Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x= MaxPooling2D((2, 2))(x)\n",
    "x= Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x= MaxPooling2D((2, 2))(x)\n",
    "encoded= Dense(512, name='encoder')(Flatten()(x))\n",
    "\n",
    "#decoding\n",
    "x= Dense(10208)(encoded)\n",
    "x= Reshape((44,29,8))(x)\n",
    "x= UpSampling2D((2, 2))(x)\n",
    "x= Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x= UpSampling2D((2, 2))(x)\n",
    "x= Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "decoded= Conv2DTranspose(3, (3,4), activation='sigmoid')(x)\n",
    "\n",
    "model= Model(input_img, decoded)\n",
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2723 samples, validate on 303 samples\n",
      "Epoch 1/30\n",
      "2723/2723 [==============================] - 170s 63ms/step - loss: 0.4542 - val_loss: 0.3500\n",
      "Epoch 2/30\n",
      "2723/2723 [==============================] - 186s 68ms/step - loss: 0.3129 - val_loss: 0.3059\n",
      "Epoch 3/30\n",
      "2723/2723 [==============================] - 198s 73ms/step - loss: 0.2864 - val_loss: 0.2869\n",
      "Epoch 4/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2697 - val_loss: 0.2697\n",
      "Epoch 5/30\n",
      "2723/2723 [==============================] - 166s 61ms/step - loss: 0.2572 - val_loss: 0.2599\n",
      "Epoch 6/30\n",
      "2723/2723 [==============================] - 165s 61ms/step - loss: 0.2473 - val_loss: 0.2539\n",
      "Epoch 7/30\n",
      "2723/2723 [==============================] - 165s 61ms/step - loss: 0.2410 - val_loss: 0.2483\n",
      "Epoch 8/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2363 - val_loss: 0.2436\n",
      "Epoch 9/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2333 - val_loss: 0.2416\n",
      "Epoch 10/30\n",
      "2723/2723 [==============================] - 165s 61ms/step - loss: 0.2301 - val_loss: 0.2395\n",
      "Epoch 11/30\n",
      "2723/2723 [==============================] - 165s 61ms/step - loss: 0.2278 - val_loss: 0.2393\n",
      "Epoch 12/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2268 - val_loss: 0.2372\n",
      "Epoch 13/30\n",
      "2723/2723 [==============================] - 165s 60ms/step - loss: 0.2249 - val_loss: 0.2360\n",
      "Epoch 14/30\n",
      "2723/2723 [==============================] - 165s 60ms/step - loss: 0.2241 - val_loss: 0.2355\n",
      "Epoch 15/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2226 - val_loss: 0.2345\n",
      "Epoch 16/30\n",
      "2723/2723 [==============================] - 167s 61ms/step - loss: 0.2215 - val_loss: 0.2338\n",
      "Epoch 17/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2217 - val_loss: 0.2343\n",
      "Epoch 18/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2206 - val_loss: 0.2328\n",
      "Epoch 19/30\n",
      "2723/2723 [==============================] - 165s 61ms/step - loss: 0.2194 - val_loss: 0.2323\n",
      "Epoch 20/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2189 - val_loss: 0.2322\n",
      "Epoch 21/30\n",
      "2723/2723 [==============================] - 164s 60ms/step - loss: 0.2183 - val_loss: 0.2319\n",
      "Epoch 22/30\n",
      "2723/2723 [==============================] - 172s 63ms/step - loss: 0.2177 - val_loss: 0.2316\n",
      "Epoch 23/30\n",
      "2723/2723 [==============================] - 187s 69ms/step - loss: 0.2171 - val_loss: 0.2309\n",
      "Epoch 24/30\n",
      "2723/2723 [==============================] - 196s 72ms/step - loss: 0.2166 - val_loss: 0.2314\n",
      "Epoch 25/30\n",
      "2723/2723 [==============================] - 168s 62ms/step - loss: 0.2167 - val_loss: 0.2317\n",
      "Epoch 26/30\n",
      "2723/2723 [==============================] - 169s 62ms/step - loss: 0.2162 - val_loss: 0.2309\n",
      "Epoch 27/30\n",
      "2723/2723 [==============================] - 169s 62ms/step - loss: 0.2158 - val_loss: 0.2309\n",
      "Epoch 28/30\n",
      "2723/2723 [==============================] - 169s 62ms/step - loss: 0.2166 - val_loss: 0.2304\n",
      "Epoch 29/30\n",
      "2723/2723 [==============================] - 168s 62ms/step - loss: 0.2152 - val_loss: 0.2299\n",
      "Epoch 30/30\n",
      "2723/2723 [==============================] - 169s 62ms/step - loss: 0.2145 - val_loss: 0.2293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2792bdc3c18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, x_train, validation_data= (x_test, x_test), epochs= 30, batch_size= 100, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= Model(inputs=model.input, outputs=model.get_layer('encoder').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x2792bdc3b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Nitin/Desktop/Capilary/train/resized_images/')\n",
    "path = \"C:/Users/Nitin/Desktop/Capilary/train/resized_images/\"\n",
    "dirs = os.listdir( path )\n",
    "x=[]\n",
    "for item in dirs:\n",
    "    img=cv2.imread(item)\n",
    "    imgarr= np.array(img)\n",
    "    x.append(imgarr.astype('float32')/255.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the feature vectors of each image\n",
    "y= encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    img1=y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.02024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "norm(y[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim=[]\n",
    "for i in range(len(y)):\n",
    "    mag_test=norm(y[i,])\n",
    "    for j in range(len(y)):\n",
    "        cos=abs(np.inner(y[j,],y[i,])/(norm(y[j,]*mag_test)))\n",
    "        cosine_sim.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9156676,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = np.reshape(cosine_sim,[3026,3026])\n",
    "cosine_sim = np.matrix(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Nitin/Desktop/Capilary/train/resized_images/')\n",
    "path = \"C:/Users/Nitin/Desktop/Capilary/train/resized_images/\"\n",
    "dirs = os.listdir( path )\n",
    "images_names=[]\n",
    "for item in dirs:\n",
    "    images_names.append(item)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(images_names)):\n",
    "    images_names[i]=images_names[i].replace(' resized.jpg','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(cosine_sim,columns=images_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image']=images_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11139192</th>\n",
       "      <th>11139194</th>\n",
       "      <th>11139524</th>\n",
       "      <th>11139560</th>\n",
       "      <th>11139588</th>\n",
       "      <th>11139650</th>\n",
       "      <th>11141306</th>\n",
       "      <th>11141308</th>\n",
       "      <th>11141318</th>\n",
       "      <th>11141320</th>\n",
       "      <th>...</th>\n",
       "      <th>14122762</th>\n",
       "      <th>14122766</th>\n",
       "      <th>14122818</th>\n",
       "      <th>14122820</th>\n",
       "      <th>14122822</th>\n",
       "      <th>14122832</th>\n",
       "      <th>14122834</th>\n",
       "      <th>14128359</th>\n",
       "      <th>14129477</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540411</td>\n",
       "      <td>0.451240</td>\n",
       "      <td>0.510148</td>\n",
       "      <td>0.310507</td>\n",
       "      <td>0.450621</td>\n",
       "      <td>0.330411</td>\n",
       "      <td>0.397326</td>\n",
       "      <td>0.426154</td>\n",
       "      <td>0.465798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354722</td>\n",
       "      <td>0.398226</td>\n",
       "      <td>0.304113</td>\n",
       "      <td>0.113903</td>\n",
       "      <td>0.312579</td>\n",
       "      <td>0.369268</td>\n",
       "      <td>0.296594</td>\n",
       "      <td>0.181255</td>\n",
       "      <td>0.538098</td>\n",
       "      <td>11139192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.225849</td>\n",
       "      <td>0.079349</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.498469</td>\n",
       "      <td>0.403348</td>\n",
       "      <td>0.580610</td>\n",
       "      <td>0.606987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>0.299994</td>\n",
       "      <td>0.161398</td>\n",
       "      <td>0.313992</td>\n",
       "      <td>0.108199</td>\n",
       "      <td>0.195023</td>\n",
       "      <td>0.060156</td>\n",
       "      <td>0.105762</td>\n",
       "      <td>0.134003</td>\n",
       "      <td>11139194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.451240</td>\n",
       "      <td>0.225849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562916</td>\n",
       "      <td>0.425020</td>\n",
       "      <td>0.902727</td>\n",
       "      <td>0.248813</td>\n",
       "      <td>0.289781</td>\n",
       "      <td>0.065774</td>\n",
       "      <td>0.208736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617220</td>\n",
       "      <td>0.491933</td>\n",
       "      <td>0.597462</td>\n",
       "      <td>0.382833</td>\n",
       "      <td>0.455781</td>\n",
       "      <td>0.685392</td>\n",
       "      <td>0.592192</td>\n",
       "      <td>0.278861</td>\n",
       "      <td>0.631222</td>\n",
       "      <td>11139524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510148</td>\n",
       "      <td>0.079349</td>\n",
       "      <td>0.562916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.379203</td>\n",
       "      <td>0.608886</td>\n",
       "      <td>0.164360</td>\n",
       "      <td>0.256186</td>\n",
       "      <td>0.125126</td>\n",
       "      <td>0.201817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484230</td>\n",
       "      <td>0.422893</td>\n",
       "      <td>0.355364</td>\n",
       "      <td>0.210983</td>\n",
       "      <td>0.373094</td>\n",
       "      <td>0.494480</td>\n",
       "      <td>0.454682</td>\n",
       "      <td>0.263318</td>\n",
       "      <td>0.688247</td>\n",
       "      <td>11139560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.310507</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.425020</td>\n",
       "      <td>0.379203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.372558</td>\n",
       "      <td>0.458899</td>\n",
       "      <td>0.520477</td>\n",
       "      <td>0.299434</td>\n",
       "      <td>0.570956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290458</td>\n",
       "      <td>0.466783</td>\n",
       "      <td>0.272180</td>\n",
       "      <td>0.263805</td>\n",
       "      <td>0.303238</td>\n",
       "      <td>0.309203</td>\n",
       "      <td>0.249277</td>\n",
       "      <td>0.254078</td>\n",
       "      <td>0.337898</td>\n",
       "      <td>11139588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.450621</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.902727</td>\n",
       "      <td>0.608886</td>\n",
       "      <td>0.372558</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.223698</td>\n",
       "      <td>0.264367</td>\n",
       "      <td>0.065040</td>\n",
       "      <td>0.151424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687165</td>\n",
       "      <td>0.505486</td>\n",
       "      <td>0.623798</td>\n",
       "      <td>0.368799</td>\n",
       "      <td>0.474039</td>\n",
       "      <td>0.755719</td>\n",
       "      <td>0.644590</td>\n",
       "      <td>0.283312</td>\n",
       "      <td>0.679517</td>\n",
       "      <td>11139650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.330411</td>\n",
       "      <td>0.498469</td>\n",
       "      <td>0.248813</td>\n",
       "      <td>0.164360</td>\n",
       "      <td>0.458899</td>\n",
       "      <td>0.223698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>0.356625</td>\n",
       "      <td>0.516032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236404</td>\n",
       "      <td>0.305673</td>\n",
       "      <td>0.250172</td>\n",
       "      <td>0.173956</td>\n",
       "      <td>0.174802</td>\n",
       "      <td>0.287442</td>\n",
       "      <td>0.190324</td>\n",
       "      <td>0.060351</td>\n",
       "      <td>0.175551</td>\n",
       "      <td>11141306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.397326</td>\n",
       "      <td>0.403348</td>\n",
       "      <td>0.289781</td>\n",
       "      <td>0.256186</td>\n",
       "      <td>0.520477</td>\n",
       "      <td>0.264367</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.358614</td>\n",
       "      <td>0.462221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253033</td>\n",
       "      <td>0.288710</td>\n",
       "      <td>0.260336</td>\n",
       "      <td>0.176272</td>\n",
       "      <td>0.179882</td>\n",
       "      <td>0.275352</td>\n",
       "      <td>0.240848</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>0.274850</td>\n",
       "      <td>11141308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.426154</td>\n",
       "      <td>0.580610</td>\n",
       "      <td>0.065774</td>\n",
       "      <td>0.125126</td>\n",
       "      <td>0.299434</td>\n",
       "      <td>0.065040</td>\n",
       "      <td>0.356625</td>\n",
       "      <td>0.358614</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.375138</td>\n",
       "      <td>0.030199</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.081355</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>0.105384</td>\n",
       "      <td>0.182890</td>\n",
       "      <td>11141318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.465798</td>\n",
       "      <td>0.606987</td>\n",
       "      <td>0.208736</td>\n",
       "      <td>0.201817</td>\n",
       "      <td>0.570956</td>\n",
       "      <td>0.151424</td>\n",
       "      <td>0.516032</td>\n",
       "      <td>0.462221</td>\n",
       "      <td>0.587257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095839</td>\n",
       "      <td>0.494979</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>0.111093</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.104438</td>\n",
       "      <td>0.030011</td>\n",
       "      <td>0.049356</td>\n",
       "      <td>0.218163</td>\n",
       "      <td>11141320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.319573</td>\n",
       "      <td>0.482579</td>\n",
       "      <td>0.271123</td>\n",
       "      <td>0.149549</td>\n",
       "      <td>0.447314</td>\n",
       "      <td>0.231564</td>\n",
       "      <td>0.558408</td>\n",
       "      <td>0.566137</td>\n",
       "      <td>0.280335</td>\n",
       "      <td>0.615908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232728</td>\n",
       "      <td>0.130458</td>\n",
       "      <td>0.243565</td>\n",
       "      <td>0.241254</td>\n",
       "      <td>0.167704</td>\n",
       "      <td>0.342292</td>\n",
       "      <td>0.214263</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>0.120434</td>\n",
       "      <td>11141324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.262676</td>\n",
       "      <td>0.326278</td>\n",
       "      <td>0.199089</td>\n",
       "      <td>0.101701</td>\n",
       "      <td>0.156685</td>\n",
       "      <td>0.157509</td>\n",
       "      <td>0.431021</td>\n",
       "      <td>0.533640</td>\n",
       "      <td>0.562933</td>\n",
       "      <td>0.178205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137772</td>\n",
       "      <td>0.055868</td>\n",
       "      <td>0.237357</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.152273</td>\n",
       "      <td>0.224052</td>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.022311</td>\n",
       "      <td>0.163377</td>\n",
       "      <td>11141326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.365313</td>\n",
       "      <td>0.168629</td>\n",
       "      <td>0.366012</td>\n",
       "      <td>0.318723</td>\n",
       "      <td>0.378355</td>\n",
       "      <td>0.387287</td>\n",
       "      <td>0.414813</td>\n",
       "      <td>0.578657</td>\n",
       "      <td>0.064146</td>\n",
       "      <td>0.244769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428922</td>\n",
       "      <td>0.123214</td>\n",
       "      <td>0.430365</td>\n",
       "      <td>0.161054</td>\n",
       "      <td>0.373752</td>\n",
       "      <td>0.387710</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.037431</td>\n",
       "      <td>0.333593</td>\n",
       "      <td>11141328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.559725</td>\n",
       "      <td>0.423888</td>\n",
       "      <td>0.502052</td>\n",
       "      <td>0.414201</td>\n",
       "      <td>0.509774</td>\n",
       "      <td>0.442461</td>\n",
       "      <td>0.377810</td>\n",
       "      <td>0.288522</td>\n",
       "      <td>0.463399</td>\n",
       "      <td>0.592063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346354</td>\n",
       "      <td>0.646415</td>\n",
       "      <td>0.280448</td>\n",
       "      <td>0.136040</td>\n",
       "      <td>0.255945</td>\n",
       "      <td>0.340866</td>\n",
       "      <td>0.249657</td>\n",
       "      <td>0.248442</td>\n",
       "      <td>0.409290</td>\n",
       "      <td>11141330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.104919</td>\n",
       "      <td>0.183018</td>\n",
       "      <td>0.120816</td>\n",
       "      <td>0.094939</td>\n",
       "      <td>0.163659</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.037630</td>\n",
       "      <td>0.055848</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051653</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>0.169237</td>\n",
       "      <td>0.122360</td>\n",
       "      <td>0.280201</td>\n",
       "      <td>0.223033</td>\n",
       "      <td>0.067130</td>\n",
       "      <td>0.163959</td>\n",
       "      <td>0.029334</td>\n",
       "      <td>11141338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.299047</td>\n",
       "      <td>0.503577</td>\n",
       "      <td>0.144267</td>\n",
       "      <td>0.092253</td>\n",
       "      <td>0.175825</td>\n",
       "      <td>0.126249</td>\n",
       "      <td>0.412871</td>\n",
       "      <td>0.430026</td>\n",
       "      <td>0.801529</td>\n",
       "      <td>0.355046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>0.170702</td>\n",
       "      <td>0.129325</td>\n",
       "      <td>0.270065</td>\n",
       "      <td>0.017225</td>\n",
       "      <td>0.142738</td>\n",
       "      <td>0.106201</td>\n",
       "      <td>0.047517</td>\n",
       "      <td>0.130332</td>\n",
       "      <td>11141340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.285681</td>\n",
       "      <td>0.193775</td>\n",
       "      <td>0.491055</td>\n",
       "      <td>0.372541</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.454154</td>\n",
       "      <td>0.303970</td>\n",
       "      <td>0.238406</td>\n",
       "      <td>0.188478</td>\n",
       "      <td>0.214775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522667</td>\n",
       "      <td>0.312320</td>\n",
       "      <td>0.486772</td>\n",
       "      <td>0.190271</td>\n",
       "      <td>0.372613</td>\n",
       "      <td>0.512902</td>\n",
       "      <td>0.387895</td>\n",
       "      <td>0.279929</td>\n",
       "      <td>0.340776</td>\n",
       "      <td>11141342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.615418</td>\n",
       "      <td>0.116305</td>\n",
       "      <td>0.398909</td>\n",
       "      <td>0.481404</td>\n",
       "      <td>0.232247</td>\n",
       "      <td>0.393575</td>\n",
       "      <td>0.092225</td>\n",
       "      <td>0.218323</td>\n",
       "      <td>0.296990</td>\n",
       "      <td>0.289721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416237</td>\n",
       "      <td>0.350066</td>\n",
       "      <td>0.187663</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.210181</td>\n",
       "      <td>0.255881</td>\n",
       "      <td>0.338468</td>\n",
       "      <td>0.163554</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>11141346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.299869</td>\n",
       "      <td>0.342601</td>\n",
       "      <td>0.230629</td>\n",
       "      <td>0.147313</td>\n",
       "      <td>0.289074</td>\n",
       "      <td>0.211752</td>\n",
       "      <td>0.676602</td>\n",
       "      <td>0.783890</td>\n",
       "      <td>0.349239</td>\n",
       "      <td>0.279426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207752</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.271244</td>\n",
       "      <td>0.198496</td>\n",
       "      <td>0.203266</td>\n",
       "      <td>0.281146</td>\n",
       "      <td>0.231689</td>\n",
       "      <td>0.032702</td>\n",
       "      <td>0.165552</td>\n",
       "      <td>11141354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.565289</td>\n",
       "      <td>0.357901</td>\n",
       "      <td>0.607836</td>\n",
       "      <td>0.508273</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.558760</td>\n",
       "      <td>0.224137</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.449079</td>\n",
       "      <td>0.324669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363134</td>\n",
       "      <td>0.438871</td>\n",
       "      <td>0.315332</td>\n",
       "      <td>0.278263</td>\n",
       "      <td>0.246833</td>\n",
       "      <td>0.464444</td>\n",
       "      <td>0.351841</td>\n",
       "      <td>0.326425</td>\n",
       "      <td>0.530759</td>\n",
       "      <td>11141530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 3027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    11139192  11139194  11139524  11139560  11139588  11139650  11141306  \\\n",
       "0   1.000000  0.540411  0.451240  0.510148  0.310507  0.450621  0.330411   \n",
       "1   0.540411  1.000000  0.225849  0.079349  0.343478  0.182765  0.498469   \n",
       "2   0.451240  0.225849  1.000000  0.562916  0.425020  0.902727  0.248813   \n",
       "3   0.510148  0.079349  0.562916  1.000000  0.379203  0.608886  0.164360   \n",
       "4   0.310507  0.343478  0.425020  0.379203  1.000000  0.372558  0.458899   \n",
       "5   0.450621  0.182765  0.902727  0.608886  0.372558  1.000000  0.223698   \n",
       "6   0.330411  0.498469  0.248813  0.164360  0.458899  0.223698  1.000000   \n",
       "7   0.397326  0.403348  0.289781  0.256186  0.520477  0.264367  0.841954   \n",
       "8   0.426154  0.580610  0.065774  0.125126  0.299434  0.065040  0.356625   \n",
       "9   0.465798  0.606987  0.208736  0.201817  0.570956  0.151424  0.516032   \n",
       "10  0.319573  0.482579  0.271123  0.149549  0.447314  0.231564  0.558408   \n",
       "11  0.262676  0.326278  0.199089  0.101701  0.156685  0.157509  0.431021   \n",
       "12  0.365313  0.168629  0.366012  0.318723  0.378355  0.387287  0.414813   \n",
       "13  0.559725  0.423888  0.502052  0.414201  0.509774  0.442461  0.377810   \n",
       "14  0.104919  0.183018  0.120816  0.094939  0.163659  0.151947  0.037630   \n",
       "15  0.299047  0.503577  0.144267  0.092253  0.175825  0.126249  0.412871   \n",
       "16  0.285681  0.193775  0.491055  0.372541  0.341400  0.454154  0.303970   \n",
       "17  0.615418  0.116305  0.398909  0.481404  0.232247  0.393575  0.092225   \n",
       "18  0.299869  0.342601  0.230629  0.147313  0.289074  0.211752  0.676602   \n",
       "19  0.565289  0.357901  0.607836  0.508273  0.314101  0.558760  0.224137   \n",
       "\n",
       "    11141308  11141318  11141320  ...  14122762  14122766  14122818  14122820  \\\n",
       "0   0.397326  0.426154  0.465798  ...  0.354722  0.398226  0.304113  0.113903   \n",
       "1   0.403348  0.580610  0.606987  ...  0.056038  0.299994  0.161398  0.313992   \n",
       "2   0.289781  0.065774  0.208736  ...  0.617220  0.491933  0.597462  0.382833   \n",
       "3   0.256186  0.125126  0.201817  ...  0.484230  0.422893  0.355364  0.210983   \n",
       "4   0.520477  0.299434  0.570956  ...  0.290458  0.466783  0.272180  0.263805   \n",
       "5   0.264367  0.065040  0.151424  ...  0.687165  0.505486  0.623798  0.368799   \n",
       "6   0.841954  0.356625  0.516032  ...  0.236404  0.305673  0.250172  0.173956   \n",
       "7   1.000000  0.358614  0.462221  ...  0.253033  0.288710  0.260336  0.176272   \n",
       "8   0.358614  1.000000  0.587257  ...  0.017711  0.375138  0.030199  0.169051   \n",
       "9   0.462221  0.587257  1.000000  ...  0.095839  0.494979  0.022633  0.111093   \n",
       "10  0.566137  0.280335  0.615908  ...  0.232728  0.130458  0.243565  0.241254   \n",
       "11  0.533640  0.562933  0.178205  ...  0.137772  0.055868  0.237357  0.275300   \n",
       "12  0.578657  0.064146  0.244769  ...  0.428922  0.123214  0.430365  0.161054   \n",
       "13  0.288522  0.463399  0.592063  ...  0.346354  0.646415  0.280448  0.136040   \n",
       "14  0.055848  0.008723  0.091171  ...  0.051653  0.057009  0.169237  0.122360   \n",
       "15  0.430026  0.801529  0.355046  ...  0.056794  0.170702  0.129325  0.270065   \n",
       "16  0.238406  0.188478  0.214775  ...  0.522667  0.312320  0.486772  0.190271   \n",
       "17  0.218323  0.296990  0.289721  ...  0.416237  0.350066  0.187663  0.004838   \n",
       "18  0.783890  0.349239  0.279426  ...  0.207752  0.069721  0.271244  0.198496   \n",
       "19  0.233200  0.449079  0.324669  ...  0.363134  0.438871  0.315332  0.278263   \n",
       "\n",
       "    14122822  14122832  14122834  14128359  14129477     image  \n",
       "0   0.312579  0.369268  0.296594  0.181255  0.538098  11139192  \n",
       "1   0.108199  0.195023  0.060156  0.105762  0.134003  11139194  \n",
       "2   0.455781  0.685392  0.592192  0.278861  0.631222  11139524  \n",
       "3   0.373094  0.494480  0.454682  0.263318  0.688247  11139560  \n",
       "4   0.303238  0.309203  0.249277  0.254078  0.337898  11139588  \n",
       "5   0.474039  0.755719  0.644590  0.283312  0.679517  11139650  \n",
       "6   0.174802  0.287442  0.190324  0.060351  0.175551  11141306  \n",
       "7   0.179882  0.275352  0.240848  0.034816  0.274850  11141308  \n",
       "8   0.081355  0.000378  0.013440  0.105384  0.182890  11141318  \n",
       "9   0.001614  0.104438  0.030011  0.049356  0.218163  11141320  \n",
       "10  0.167704  0.342292  0.214263  0.009065  0.120434  11141324  \n",
       "11  0.152273  0.224052  0.210107  0.022311  0.163377  11141326  \n",
       "12  0.373752  0.387710  0.511508  0.037431  0.333593  11141328  \n",
       "13  0.255945  0.340866  0.249657  0.248442  0.409290  11141330  \n",
       "14  0.280201  0.223033  0.067130  0.163959  0.029334  11141338  \n",
       "15  0.017225  0.142738  0.106201  0.047517  0.130332  11141340  \n",
       "16  0.372613  0.512902  0.387895  0.279929  0.340776  11141342  \n",
       "17  0.210181  0.255881  0.338468  0.163554  0.580873  11141346  \n",
       "18  0.203266  0.281146  0.231689  0.032702  0.165552  11141354  \n",
       "19  0.246833  0.464444  0.351841  0.326425  0.530759  11141530  \n",
       "\n",
       "[20 rows x 3027 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cosine_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/Nitin/Desktop/Capilary/train/cosine_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods=[]\n",
    "df1=df\n",
    "for i in df1.drop(['image'],axis=1).columns:\n",
    "    df1=df1.sort_values([i],ascending=[0])\n",
    "    prods.append(df1[['image']].values[1:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3026"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Recommend2=pd.DataFrame(np.array(prods).reshape(3026,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Recommend2['Product'] = df1.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12658520</td>\n",
       "      <td>11659366</td>\n",
       "      <td>13038884</td>\n",
       "      <td>12657640</td>\n",
       "      <td>12407248</td>\n",
       "      <td>12657642</td>\n",
       "      <td>11714484</td>\n",
       "      <td>12657550</td>\n",
       "      <td>11139192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12657630</td>\n",
       "      <td>14122744</td>\n",
       "      <td>13017332</td>\n",
       "      <td>13017336</td>\n",
       "      <td>11407364</td>\n",
       "      <td>12657638</td>\n",
       "      <td>13064964</td>\n",
       "      <td>11659280</td>\n",
       "      <td>11139194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11139650</td>\n",
       "      <td>11145614</td>\n",
       "      <td>11147546</td>\n",
       "      <td>11147548</td>\n",
       "      <td>12437090</td>\n",
       "      <td>11145602</td>\n",
       "      <td>11144136</td>\n",
       "      <td>11145642</td>\n",
       "      <td>11139524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12658504</td>\n",
       "      <td>11659562</td>\n",
       "      <td>11659208</td>\n",
       "      <td>12657378</td>\n",
       "      <td>11714570</td>\n",
       "      <td>12658246</td>\n",
       "      <td>12360396</td>\n",
       "      <td>12658614</td>\n",
       "      <td>11139560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11659362</td>\n",
       "      <td>12657606</td>\n",
       "      <td>11660048</td>\n",
       "      <td>11148440</td>\n",
       "      <td>11141320</td>\n",
       "      <td>12988620</td>\n",
       "      <td>11603772</td>\n",
       "      <td>11407112</td>\n",
       "      <td>11139588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  12658520  11659366  13038884  12657640  12407248  12657642  11714484   \n",
       "1  12657630  14122744  13017332  13017336  11407364  12657638  13064964   \n",
       "2  11139650  11145614  11147546  11147548  12437090  11145602  11144136   \n",
       "3  12658504  11659562  11659208  12657378  11714570  12658246  12360396   \n",
       "4  11659362  12657606  11660048  11148440  11141320  12988620  11603772   \n",
       "\n",
       "          7   Product  \n",
       "0  12657550  11139192  \n",
       "1  11659280  11139194  \n",
       "2  11145642  11139524  \n",
       "3  12658614  11139560  \n",
       "4  11407112  11139588  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Recommend2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Recommend2.to_csv('C:/Users/Nitin/Desktop/Capilary/Product_based_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
